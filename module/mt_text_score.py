
import nltk.translate.gleu_score as gleu
import nltk.translate.bleu_score as bleu
import glob
import argparse

__author__ = "Gwena Cunha"


""" Class that provides methods to calculate similarity between two texts

    Macro-average precision: Average of sentence scores
    Micro-average precision: Corpus (sums numerators and denominators for each hypothesis-reference(s) pairs before division)

    Metrics for Machine Translation performance:
        1. BLEU (Bilingual Evaluation Understudy) score limitation (Papineni 2002): designed to be a corpus measure, so
            it has undesirable properties when used for single sentences. 'Measures how many words overlap in a given
             translation when compared to a reference translation, giving higher scores to sequential words.'
            Paper: https://www.aclweb.org/anthology/P02-1040.pdf
        2. GLEU (Google-BLEU) score (Wu et al. 2016): minimum of BLEU recall and precision applied to 1, 2, 3 and 4grams
            Recall: (number of matching n-grams) / (number of total n-grams in the target)
            Precision: (number of matching n-grams) / (number of total n-grams in generated sequence)
        3. Word error rate (WER)
        4. Skip-gram: similar to word embedding
        5. METEOR (Metric for Evaluation of Translation with Explicit ORdering):
        6. Translation Edit Rate (TER)
        7. char-TER
        8. General Text Matcher (GTM)

    TODO in new script:
    * Metrics for Summarization performance:
        1. ROUGE (Recall-Oriented Understudy for Gisting Evaluation):

    Source: http://www.nltk.org/api/nltk.translate.html
"""


class TextScore:

    def __init__(self):
        print("Initialize Machine Translation text score")

    def score_multiple_from_file(self, ref_file, hyp_file, scores_file, score_type="BLEU", average_prec="corpus"):
        # Clean scores_file if existent
        open(scores_file, 'w').close()

        scores = []
        if "BLEU" in score_type:
            scores.append(self.score_one_from_file(ref_file, hyp_file, scores_file, score_type="BLEU", average_prec=average_prec))

        if "GLEU" in score_type:
            scores.append(self.score_one_from_file(ref_file, hyp_file, scores_file, score_type="GLEU", average_prec=average_prec))

        return scores

    def score_one_from_file(self, ref_file, hyp_file, scores_file, score_type="BLEU", average_prec="corpus"):
        """ Calculates score of file where each re line is a text corresponding to the same hyp line
        Doesn't treat cases of multiple references for the same hypotheses

        :param ref_file: text file of reference sentences
        :param hyp_file: text file of sentences generated by model
        :param scores_file: text file with scores
        :param score_type: BLEU or GLEU
        :param average_prec: "corpus", "sent_average" or both ("corpus sent_average")
        :return: final score
        """

        hf = open(hyp_file, "r")
        hypothesis = hf.read().split("\n")
        num_sentences = len(hypothesis) - 1

        rf = open(ref_file, "r")
        reference = rf.read().split("\n")

        sf = open(scores_file, "a+")

        list_of_references = []
        hypotheses = []
        real_num_sentences = 0
        for i in range(0, num_sentences):
            if len(reference[i].strip()) != 0 or len(hypothesis[i].strip()) != 0:
                ref, hypo = reference[i].lower().split(), hypothesis[i].lower().split()
                list_of_references.append([ref])
                hypotheses.append(hypo)
                real_num_sentences += 1

        print("Sentences: " + str(real_num_sentences))
        scores_str = ""
        score_corpus, score_sent = None, None
        if "corpus" in average_prec:
            score_corpus = self.corpus_score(list_of_references, hypotheses, score_type=score_type)
            scores_str += score_type + " corpus: " + str(score_corpus) + "\n"
        if "sent_average" in average_prec:
            score_sent = self.sentence_average_score(list_of_references, hypotheses, score_type=score_type)
            scores_str += score_type + " sent_average: " + str(score_sent) + "\n"

        scores_str += "\n"
        sf.write(scores_str)
        sf.close()

        return score_corpus, score_sent

    def corpus_score(self, list_of_references, hypotheses, score_type="BLEU"):
        """ Score specifically implemented for corpus

        :return:
        """

        if "BLEU" in score_type:
            corpus_score = bleu.corpus_bleu(list_of_references, hypotheses)
        else:  # "GLEU"
            corpus_score = gleu.corpus_gleu(list_of_references, hypotheses)
        print("%s corpus score: %.4f" % (score_type, corpus_score))
        return corpus_score

    def sentence_average_score(self, list_of_references, hypotheses, score_type="BLEU"):
        """ Averages score applied for every sentence

        :return:
        """

        sent_average_score = 0
        if "BLEU" in score_type:
            for ref, hyp in zip(list_of_references, hypotheses):
                sent_average_score += bleu.sentence_bleu(ref, hyp)  # gram: default is between 1 and 4
        else:  # "GLEU
            for ref, hyp in zip(list_of_references, hypotheses):
                sent_average_score += gleu.sentence_gleu(ref, hyp)  # gram: default is between 1 and 4

        sent_average_score /= len(list_of_references)

        print("%s sentence average score: %.4f" % (score_type, sent_average_score))
        return sent_average_score

    def meteor(self):
        # TODO: Meteor metric
        print("TODO: METEOR text score")

