{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bleu.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gcunhase/NLPMetrics/blob/master/notebooks/bleu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cEvHsd4OnnO2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BLEU: BiLingual Evaluation Understudy\n",
        "\n",
        "*NLP evaluation metric used in Machine Translation tasks*\n",
        "\n",
        "*$N$-gram comparison between words in candidate sentence and reference sentences*"
      ]
    },
    {
      "metadata": {
        "id": "_arqa6LRnzCL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Libraries\n",
        "*Install and import necessary libraries*\n"
      ]
    },
    {
      "metadata": {
        "id": "xFOnk5JdnuYQ",
        "colab_type": "code",
        "outputId": "05a9a19e-4170-4280-9dc8-d6bb967650c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu\n",
        "\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "try:\n",
        "  nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "  nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SVkfsYSZq_zn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Dataset\n",
        "*Array of words: candidate and reference sentences split into words*"
      ]
    },
    {
      "metadata": {
        "id": "Dr9v92X0r9VM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hyp = str('she read the book because she was interested in world history').split()\n",
        "ref_a = str('she read the book because she was interested in world history').split()\n",
        "ref_b = str('she was interested in world history because she read the book').split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PQYjMHOgsyfT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. *Sentence* score calculation\n",
        "*Compares 1 hypothesis (candidate or source sentence) and 1+ reference sentences.*\n",
        "\n",
        "* *hyp and ref_a are the same so the BLEU score should be 1*"
      ]
    },
    {
      "metadata": {
        "id": "jXGCD-pi-jt5",
        "colab_type": "code",
        "outputId": "b183fde2-c015-45af-d9c4-a487e70353ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "score_ref_a = bleu.sentence_bleu([ref_a], hyp)\n",
        "print(score_ref_a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x91PWBD3AQ3w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* *hyp and ref_b are different, so the BLEU score should be lower than 1*"
      ]
    },
    {
      "metadata": {
        "id": "dl1hUnAgAVMH",
        "colab_type": "code",
        "outputId": "36bc0c9c-4a72-4cbb-deb7-b08fb5a7c5d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "score_ref_b = bleu.sentence_bleu([ref_b], hyp)\n",
        "print(score_ref_b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7400828044922853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9HUt9fi5AZvu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* *When comparing a candidate sentence with multiple reference sentences, the function will return the highest score*"
      ]
    },
    {
      "metadata": {
        "id": "zoOQKytIAaBw",
        "colab_type": "code",
        "outputId": "cea1ee7c-b53e-4258-81e8-e8fc1514102f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "score_ref_ab = bleu.sentence_bleu([ref_a, ref_b], hyp)\n",
        "print(score_ref_ab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NW9ZXSsSs6bE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. *Corpus* score calculation\n",
        "*Compares 1 candidate document with multiple sentence and 1+ reference documents also with multiple sentences.*\n",
        "\n",
        "* Different than averaging BLEU scores of each sentence, it calculates the score by *\"summing the numerators and denominators for each hypothesis-reference(s) pairs before the division\"*"
      ]
    },
    {
      "metadata": {
        "id": "XATgeqKPP02p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score_ref_a = bleu.corpus_bleu([[ref_a]], [hyp])\n",
        "print(\"1 document with 1 reference sentence: {}\".format(score_ref_a))\n",
        "score_ref_a = bleu.corpus_bleu([[ref_a, ref_b]], [hyp])\n",
        "print(\"1 document with 2 reference sentences: {}\".format(score_ref_a))\n",
        "score_ref_a = bleu.corpus_bleu([[ref_a], [ref_b]], [hyp, hyp])\n",
        "print(\"2 documents with 1 reference sentence each: {}\".format(score_ref_a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hxgDToMctnTM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5. N-gram\n",
        "*N-gram scores can be obtained in both **sentence** and **corpus** calculations and they're indicated by the **weights** parameter.*\n",
        "\n",
        "* *weights*: length 4, where each index contains a weight corresponding to its respective N-gram.\n",
        "* N-gram with $N \\in \\{1, 2, 3, 4\\}$\n",
        "* $\\textit{weights}=(W_{N=1}, W_{N=2}, W_{N=3}, W_{N=4})$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0J2_E8zQP6K9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score_1gram = bleu.sentence_bleu([ref_a], hyp, weights=(1,0,0,0))\n",
        "score_2gram = bleu.sentence_bleu([ref_a], hyp, weights=(0,1,0,0))\n",
        "score_3gram = bleu.sentence_bleu([ref_a], hyp, weights=(0,0,1,0))\n",
        "score_4gram = bleu.sentence_bleu([ref_a], hyp, weights=(0,0,0,1))\n",
        "print(\"N-grams: 1-{}, 2-{}, 3-{}, 4-{}\".format(score_1gram, score_2gram, score_3gram, score_4gram))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "El1PaLtIDQyH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Cumulative N-grams: *by default, the score is calculatedby considering all $N$-grams equally*"
      ]
    },
    {
      "metadata": {
        "id": "ntJ1UkEaP-90",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score_ngram1 = bleu.sentence_bleu([ref_b], hyp)\n",
        "score_ngram = bleu.sentence_bleu([ref_b], hyp, weights=(0.25,0.25,0.25,0.25))\n",
        "print(\"N-grams: {}, {}\".format(score_ngram1, score_ngram))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}